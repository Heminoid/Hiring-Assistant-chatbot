{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install gradio openai python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uBWWc56ByVD",
        "outputId": "159b9d31-5bdf-493a-a644-6d8480784852"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, markupsafe, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.21.0 gradio-client-1.7.2 groovy-0.1.2 markupsafe-2.1.5 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.20 ruff-0.11.0 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and Setup Environment"
      ],
      "metadata": {
        "id": "btDOj-HhBuV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import datetime\n",
        "import uuid\n",
        "import time\n",
        "from openai import OpenAI  # Updated import for OpenAI v1.0.0+\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configure OpenAI API with the new client approach\n",
        "#client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "# If you don't have .env file, uncomment and add your key directly:\n",
        "client = OpenAI(api_key=\"your api key\")"
      ],
      "metadata": {
        "id": "PAMijDYwBscL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Conversation States and Data Structures"
      ],
      "metadata": {
        "id": "1xrHyboqBoxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversation states\n",
        "class ConversationState:\n",
        "    GREETING = 0\n",
        "    NAME = 1\n",
        "    EMAIL = 2\n",
        "    PHONE = 3\n",
        "    EXPERIENCE = 4\n",
        "    POSITION = 5\n",
        "    LOCATION = 6\n",
        "    TECH_STACK = 7\n",
        "    TECHNICAL_QUESTIONS = 8\n",
        "    CONCLUSION = 9\n",
        "\n",
        "# Candidate data structure\n",
        "class Candidate:\n",
        "    def __init__(self):\n",
        "        self.id = str(uuid.uuid4())\n",
        "        self.name = \"\"\n",
        "        self.email = \"\"\n",
        "        self.phone = \"\"\n",
        "        self.experience = \"\"\n",
        "        self.position = \"\"\n",
        "        self.location = \"\"\n",
        "        self.tech_stack = \"\"\n",
        "        self.interview_date = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        self.technical_responses = []\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            \"id\": self.id,\n",
        "            \"name\": self.name,\n",
        "            \"email\": self.email,\n",
        "            \"phone\": self.phone,\n",
        "            \"experience\": self.experience,\n",
        "            \"position\": self.position,\n",
        "            \"location\": self.location,\n",
        "            \"tech_stack\": self.tech_stack,\n",
        "            \"interview_date\": self.interview_date,\n",
        "            \"technical_responses\": self.technical_responses\n",
        "        }\n",
        "\n",
        "    def save_to_json(self, filename=None):\n",
        "        if not filename:\n",
        "            filename = f\"candidate_{self.id}.json\"\n",
        "        try:\n",
        "            with open(filename, 'w') as f:\n",
        "                json.dump(self.to_dict(), f, indent=4)\n",
        "            print(f\"Saved candidate data to {filename}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving candidate data: {e}\")\n",
        "            return False\n"
      ],
      "metadata": {
        "id": "P1XcPtZDBmb8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Prompt Templates"
      ],
      "metadata": {
        "id": "-m72QJdzBkwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# System prompt to define the chatbot's behavior\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an AI hiring assistant for TalentScout, a recruitment agency specializing in technology placements.\n",
        "Your role is to gather essential information from candidates and assess their technical skills.\n",
        "Be professional, courteous, and focused on the recruitment process.\n",
        "Keep responses concise and relevant to the hiring context.\n",
        "DO NOT ask multiple questions at once - focus on one information request at a time.\n",
        "DO NOT make up information about the candidate.\n",
        "\"\"\"\n",
        "\n",
        "# Greeting prompt\n",
        "GREETING_PROMPT = \"\"\"\n",
        "Introduce yourself as TalentScout's AI hiring assistant. Explain that you'll be collecting some\n",
        "basic information and then asking a few technical questions based on their skills.\n",
        "Ask for their full name to begin the process.\n",
        "\"\"\"\n",
        "\n",
        "# Tech stack prompt\n",
        "TECH_STACK_PROMPT = \"\"\"\n",
        "Ask the candidate to list their technical skills, including programming languages, frameworks,\n",
        "databases, and tools they are proficient in. Request them to be specific and comprehensive.\n",
        "\"\"\"\n",
        "\n",
        "# Technical questions prompt - Improved for better questions\n",
        "TECHNICAL_QUESTIONS_PROMPT = \"\"\"\n",
        "Based on the candidate's tech stack: {tech_stack}, generate 3-5 relevant technical questions\n",
        "that would help assess their proficiency. Follow these guidelines:\n",
        "1. Questions should be specific to the exact technologies mentioned (not generic)\n",
        "2. Include a mix of knowledge-based questions and problem-solving scenarios\n",
        "3. Questions should be appropriate for someone with {years_experience} years of experience\n",
        "4. For programming languages, include at least one algorithmic or coding challenge\n",
        "5. For frameworks, include questions about best practices and common pitfalls\n",
        "6. For databases, include questions about optimization and data modeling\n",
        "\n",
        "Ask one question at a time and wait for the response before asking the next question.\n",
        "Start with an easier question and gradually increase difficulty.\n",
        "\"\"\"\n",
        "\n",
        "# Conclusion prompt\n",
        "CONCLUSION_PROMPT = \"\"\"\n",
        "Thank the candidate for their time and participation. Inform them that TalentScout will\n",
        "review their information and be in touch if they're a good match for available positions.\n",
        "Wish them well in their job search.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "fgCG5kG8Bg0P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Utility Functions"
      ],
      "metadata": {
        "id": "BFvJWFI0Ber0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation functions\n",
        "def validate_email(email):\n",
        "    \"\"\"Validate email format\"\"\"\n",
        "    pattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$'\n",
        "    return re.match(pattern, email) is not None\n",
        "\n",
        "def validate_phone(phone):\n",
        "    \"\"\"Validate phone number format\"\"\"\n",
        "    pattern = r'^\\+?[0-9]{10,15}$'\n",
        "    return re.match(pattern, phone) is not None\n",
        "\n",
        "# Function to interact with OpenAI API with error handling - Updated for OpenAI v1.0.0+\n",
        "def get_completion(messages, model=\"gpt-3.5-turbo\", temperature=0.7, retries=3):\n",
        "    \"\"\"Call the LLM with error handling and retries\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=messages,\n",
        "                temperature=temperature\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            if attempt == retries - 1:\n",
        "                # Last attempt failed, return a fallback response\n",
        "                print(f\"Error calling LLM API: {e}\")\n",
        "                return \"I'm having trouble connecting right now. Please try again, or provide your response to the last question.\"\n",
        "            # Wait before retrying (exponential backoff)\n",
        "            time.sleep(2 ** attempt)\n",
        "\n",
        "# Function to check for conversation ending keywords\n",
        "def is_conversation_ending(message):\n",
        "    \"\"\"Check if the message contains conversation-ending keywords\"\"\"\n",
        "    keywords = [\"goodbye\", \"bye\", \"exit\", \"quit\", \"end\"]\n",
        "    return any(keyword in message.lower() for keyword in keywords)\n",
        "\n",
        "# Function to get appropriate reprompt for a given state\n",
        "def get_reprompt(state):\n",
        "    \"\"\"Get an appropriate reprompt for the given conversation state\"\"\"\n",
        "    prompts = {\n",
        "        ConversationState.NAME: \"Could you please provide your full name?\",\n",
        "        ConversationState.EMAIL: \"Please provide a valid email address where we can reach you.\",\n",
        "        ConversationState.PHONE: \"Please provide a valid phone number where we can reach you.\",\n",
        "        ConversationState.EXPERIENCE: \"How many years of professional experience do you have?\",\n",
        "        ConversationState.POSITION: \"What position(s) are you interested in applying for?\",\n",
        "        ConversationState.LOCATION: \"What is your current location?\",\n",
        "        ConversationState.TECH_STACK: \"Please list your technical skills, including programming languages, frameworks, databases, and tools.\"\n",
        "    }\n",
        "    return prompts.get(state, \"Could you please provide the information I requested?\")\n"
      ],
      "metadata": {
        "id": "PMzmaML5BaSr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Tech Stack Analyzer Function"
      ],
      "metadata": {
        "id": "C-GGJAdVBW3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_tech_stack(tech_stack_text):\n",
        "    \"\"\"Analyze and categorize the candidate's tech stack\"\"\"\n",
        "    tech_analysis_prompt = f\"\"\"\n",
        "    Analyze this tech stack: \"{tech_stack_text}\"\n",
        "\n",
        "    1. Identify all technologies mentioned\n",
        "    2. Categorize them into: Programming Languages, Frameworks, Databases, Cloud Services, Tools\n",
        "    3. For each identified technology, note the appropriate experience level to ask questions about (beginner, intermediate, advanced)\n",
        "\n",
        "    Return your analysis in a structured JSON format.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        analysis_messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a technical recruiter analyzing candidate skills.\"},\n",
        "            {\"role\": \"user\", \"content\": tech_analysis_prompt}\n",
        "        ]\n",
        "        analysis_result = get_completion(analysis_messages)\n",
        "\n",
        "        return analysis_result\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing tech stack: {e}\")\n",
        "        return tech_stack_text\n"
      ],
      "metadata": {
        "id": "oW3p33ueBUm0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement the Core Conversation Handler"
      ],
      "metadata": {
        "id": "nn5kG0yABRv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize global variables\n",
        "conversation_state = ConversationState.GREETING\n",
        "candidate = Candidate()\n",
        "messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "question_count = 0\n",
        "current_tech_question = \"\"\n",
        "tech_stack_analysis = \"\"\n",
        "\n",
        "def chat_with_bot(message, history):\n",
        "    global conversation_state, candidate, messages, question_count, current_tech_question, tech_stack_analysis\n",
        "\n",
        "    # Check for conversation-ending keywords\n",
        "    if is_conversation_ending(message):\n",
        "        # Save candidate data before ending\n",
        "        try:\n",
        "            if candidate.name and candidate.email:  # Only save if we have basic info\n",
        "                candidate.save_to_json()\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving candidate data: {e}\")\n",
        "\n",
        "        # Return message in the correct format for messages type chatbot\n",
        "        return [{\"role\": \"assistant\", \"content\": \"Thank you for your time. The conversation has been ended. Feel free to refresh the page to start a new chat.\"}]\n",
        "\n",
        "    # Add user message to messages history\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Process based on conversation state\n",
        "    if conversation_state == ConversationState.GREETING:\n",
        "        if not history:  # First message\n",
        "            bot_response = get_completion([\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": GREETING_PROMPT}\n",
        "            ])\n",
        "        else:\n",
        "            candidate.name = message\n",
        "            bot_response = \"Thank you. Now, please provide your email address.\"\n",
        "            conversation_state = ConversationState.EMAIL\n",
        "\n",
        "    elif conversation_state == ConversationState.EMAIL:\n",
        "        if validate_email(message):\n",
        "            candidate.email = message\n",
        "            bot_response = \"Great! Could you share your phone number, please?\"\n",
        "            conversation_state = ConversationState.PHONE\n",
        "        else:\n",
        "            bot_response = \"That doesn't look like a valid email address. Please provide a valid email in the format name@example.com.\"\n",
        "\n",
        "    elif conversation_state == ConversationState.PHONE:\n",
        "        if validate_phone(message):\n",
        "            candidate.phone = message\n",
        "            bot_response = \"How many years of professional experience do you have in the technology field?\"\n",
        "            conversation_state = ConversationState.EXPERIENCE\n",
        "        else:\n",
        "            bot_response = \"That doesn't look like a valid phone number. Please provide a valid phone number (10-15 digits, can start with +).\"\n",
        "\n",
        "    elif conversation_state == ConversationState.EXPERIENCE:\n",
        "        candidate.experience = message\n",
        "        bot_response = \"What position(s) are you interested in applying for at TalentScout?\"\n",
        "        conversation_state = ConversationState.POSITION\n",
        "\n",
        "    elif conversation_state == ConversationState.POSITION:\n",
        "        candidate.position = message\n",
        "        bot_response = \"What is your current location or preferred work location?\"\n",
        "        conversation_state = ConversationState.LOCATION\n",
        "\n",
        "    elif conversation_state == ConversationState.LOCATION:\n",
        "        candidate.location = message\n",
        "        bot_response = get_completion(messages + [\n",
        "            {\"role\": \"assistant\", \"content\": TECH_STACK_PROMPT}\n",
        "        ])\n",
        "        conversation_state = ConversationState.TECH_STACK\n",
        "\n",
        "    elif conversation_state == ConversationState.TECH_STACK:\n",
        "        candidate.tech_stack = message\n",
        "\n",
        "        # Analyze tech stack (this doesn't affect the flow but will be saved)\n",
        "        tech_stack_analysis = analyze_tech_stack(message)\n",
        "\n",
        "        # Generate the first technical question\n",
        "        tech_question_prompt = TECHNICAL_QUESTIONS_PROMPT.format(\n",
        "            tech_stack=candidate.tech_stack,\n",
        "            years_experience=candidate.experience\n",
        "        )\n",
        "\n",
        "        # Add context about the purpose of these questions\n",
        "        intro_message = \"Thank you for sharing your technical background. I'll now ask you a few technical questions based on your skills to better understand your expertise level.\"\n",
        "\n",
        "        tech_questions_response = get_completion(messages + [\n",
        "            {\"role\": \"user\", \"content\": tech_question_prompt}\n",
        "        ])\n",
        "\n",
        "        bot_response = intro_message + \"\\n\\n\" + tech_questions_response\n",
        "        conversation_state = ConversationState.TECHNICAL_QUESTIONS\n",
        "        question_count = 1\n",
        "        current_tech_question = tech_questions_response\n",
        "\n",
        "    elif conversation_state == ConversationState.TECHNICAL_QUESTIONS:\n",
        "        # Save the candidate's response to the current question\n",
        "        candidate.technical_responses.append({\n",
        "            \"question\": current_tech_question,\n",
        "            \"answer\": message\n",
        "        })\n",
        "\n",
        "        question_count += 1\n",
        "\n",
        "        # Check if we've asked enough questions\n",
        "        if question_count > 3:  # Limiting to 3 questions\n",
        "            # Move to conclusion\n",
        "            conclusion_message = get_completion(messages + [\n",
        "                {\"role\": \"user\", \"content\": CONCLUSION_PROMPT}\n",
        "            ])\n",
        "\n",
        "            bot_response = conclusion_message\n",
        "\n",
        "            # Save candidate data\n",
        "            try:\n",
        "                candidate.save_to_json()\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving candidate data: {e}\")\n",
        "\n",
        "            conversation_state = ConversationState.CONCLUSION\n",
        "        else:\n",
        "            # Ask the next question\n",
        "            next_question_prompt = f\"\"\"\n",
        "            Based on the candidate's tech stack ({candidate.tech_stack}) and their experience level ({candidate.experience} years),\n",
        "            ask technical question #{question_count}. Make it more challenging than the previous question.\n",
        "            Tailor this question to assess deeper knowledge or problem-solving skills.\n",
        "            \"\"\"\n",
        "\n",
        "            next_question = get_completion(messages + [\n",
        "                {\"role\": \"user\", \"content\": next_question_prompt}\n",
        "            ])\n",
        "\n",
        "            bot_response = next_question\n",
        "            current_tech_question = next_question\n",
        "\n",
        "    elif conversation_state == ConversationState.CONCLUSION:\n",
        "        bot_response = \"Thank you again for your time. The interview is now complete. The TalentScout team will review your responses and contact you if there's a good match. Feel free to refresh the page to start a new conversation.\"\n",
        "\n",
        "    # Add bot response to messages\n",
        "    messages.append({\"role\": \"assistant\", \"content\": bot_response})\n",
        "\n",
        "    # Return in format compatible with messages type chatbot\n",
        "    return [{\"role\": \"assistant\", \"content\": bot_response}]\n"
      ],
      "metadata": {
        "id": "GOQvwhB2BOGr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Reset Function"
      ],
      "metadata": {
        "id": "z5IcgWf9BJhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset function to start a new conversation\n",
        "def reset_conversation():\n",
        "    global conversation_state, candidate, messages, question_count, current_tech_question, tech_stack_analysis\n",
        "\n",
        "    # Reset all state variables\n",
        "    conversation_state = ConversationState.GREETING\n",
        "    candidate = Candidate()\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    question_count = 0\n",
        "    current_tech_question = \"\"\n",
        "    tech_stack_analysis = \"\"\n",
        "\n",
        "    # Get initial greeting\n",
        "    initial_greeting = get_completion([\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": GREETING_PROMPT}\n",
        "    ])\n",
        "\n",
        "    # Return in format compatible with messages type chatbot\n",
        "    return [{\"role\": \"assistant\", \"content\": initial_greeting}]\n"
      ],
      "metadata": {
        "id": "1Aw3qOjgA8hp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Gradio Interface with Experience Selection Radio Buttons"
      ],
      "metadata": {
        "id": "98sM5_SdBDKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced CSS for a more attractive, two-column design\n",
        "custom_css = \"\"\"\n",
        "/* Core styles */\n",
        "footer {visibility: hidden}\n",
        ".gradio-container {max-width: 1100px; margin: 0 auto;}\n",
        "\n",
        "/* Two-column layout styling */\n",
        ".container {\n",
        "    display: flex;\n",
        "    gap: 20px;\n",
        "}\n",
        "\n",
        ".left-panel {\n",
        "    background-color: Black;\n",
        "    border-radius: 12px;\n",
        "    padding: 15px;\n",
        "    box-shadow: 0 2px 6px rgba(0,0,0,0.1);\n",
        "}\n",
        "\n",
        ".chat-panel {\n",
        "    flex-grow: 1;\n",
        "    border-radius: 12px;\n",
        "    box-shadow: 0 2px 6px rgba(0,0,0,0.1);\n",
        "}\n",
        "\n",
        "/* Message styling with guaranteed visibility */\n",
        ".message.user {\n",
        "    background-color: black !important;\n",
        "    color: white !important;\n",
        "    border-radius: 12px !important;\n",
        "    padding: 12px !important;\n",
        "    margin: 8px 0 !important;\n",
        "    max-width: 85% !important;\n",
        "    margin-left: auto !important;\n",
        "}\n",
        "\n",
        ".message.bot {\n",
        "    background-color: black !important;\n",
        "    color: white !important;\n",
        "    border-radius: 12px !important;\n",
        "    padding: 12px !important;\n",
        "    margin: 8px 0 !important;\n",
        "    max-width: 85% !important;\n",
        "    margin-right: auto !important;\n",
        "}\n",
        "\n",
        "/* Input area styling */\n",
        ".input-area {\n",
        "    padding: 12px;\n",
        "    border-top: 1px solid #e2e8f0;\n",
        "}\n",
        "\n",
        "/* Button styling */\n",
        ".primary-btn {\n",
        "    background-color: #2563eb !important;\n",
        "    color: white !important;\n",
        "    border-radius: 8px !important;\n",
        "}\n",
        "\n",
        ".secondary-btn {\n",
        "    background-color: #64748b !important;\n",
        "    color: white !important;\n",
        "    border-radius: 8px !important;\n",
        "}\n",
        "\n",
        "/* Radio button styling */\n",
        ".radio-group .wrap {\n",
        "    display: grid !important;\n",
        "    grid-template-columns: 1fr 1fr;\n",
        "    text-align: left;\n",
        "    gap: 8px;\n",
        "}\n",
        "\n",
        ".radio-group label {\n",
        "    padding: 8px !important;\n",
        "    background-color: black !important;\n",
        "    border: 1px solid #e2e8f0 !important;\n",
        "    border-radius: 6px !important;\n",
        "}\n",
        "\n",
        ".radio-group input[type=\"radio\"]:checked + label {\n",
        "    background-color: #bfdbfe !important;\n",
        "    border-color: #3b82f6 !important;\n",
        "}\n",
        "\n",
        "/* Info box styling */\n",
        ".info-box {\n",
        "    background-color: #27272a;\n",
        "    border-left: 3px solid #2563eb;\n",
        "    padding: 10px;\n",
        "    margin: 10px 0;\n",
        "    border-radius: 4px;\n",
        "}\n",
        "\"\"\"\n",
        "# Experience level options for radio buttons\n",
        "EXPERIENCE_OPTIONS = [\"0-1 years\", \"1-3 years\", \"3-5 years\", \"5-10 years\", \"10+ years\"]\n",
        "\n",
        "# Create enhanced two-column interface\n",
        "with gr.Blocks(theme=gr.themes.Base(), css=custom_css) as demo:\n",
        "    gr.Markdown(\"# ðŸ¤– TalentScout Hiring Assistant\")\n",
        "\n",
        "    # Two-column layout\n",
        "    with gr.Row():\n",
        "        # Left column for profile and controls\n",
        "        with gr.Column(scale=1, elem_classes=\"left-panel\"):\n",
        "            gr.Markdown(\"### Candidate Profile\")\n",
        "\n",
        "            # Experience selection\n",
        "            gr.Markdown(\"**Experience Level**\")\n",
        "            experience_radio = gr.Radio(\n",
        "                choices=EXPERIENCE_OPTIONS,\n",
        "                label=\"\",\n",
        "                info=\"Select your years of experience\",\n",
        "                elem_classes=\"radio-group\"\n",
        "            )\n",
        "\n",
        "\n",
        "            # Info box with tips\n",
        "            with gr.Column(elem_classes=\"info-box\"):\n",
        "                gr.Markdown(\"\"\"\n",
        "                ### Interview Tips\n",
        "                â€¢ Be specific about your technical skills\n",
        "\n",
        "                â€¢ Provide examples from past projects\n",
        "\n",
        "                â€¢ Be honest about your proficiency levels\n",
        "\n",
        "                â€¢ Ask for clarification if needed\n",
        "                \"\"\")\n",
        "\n",
        "                # Function to handle experience selection via radio buttons\n",
        "        def handle_experience_selection(choice):\n",
        "            global conversation_state, candidate\n",
        "\n",
        "            candidate.experience = choice\n",
        "            conversation_state = ConversationState.POSITION\n",
        "\n",
        "            response = f\"Thank you for selecting {choice} of experience. What position(s) are you interested in applying for at TalentScout?\"\n",
        "            messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "            return [{\"role\": \"assistant\", \"content\": response}]\n",
        "\n",
        "            # Reset button\n",
        "            reset_btn = gr.Button(\"Reset Conversation\", elem_classes=[\"secondary-btn\"])\n",
        "\n",
        "        # Right column for chat\n",
        "        with gr.Column(scale=2, elem_classes=\"chat-panel\"):\n",
        "            # Chat display\n",
        "            chatbot = gr.Chatbot(\n",
        "                height=500,\n",
        "                value=[],\n",
        "                type=\"messages\",\n",
        "                show_label=False\n",
        "            )\n",
        "\n",
        "            # Input area with send button\n",
        "            with gr.Row(elem_classes=\"input-area\"):\n",
        "                msg = gr.Textbox(\n",
        "                    placeholder=\"Type your message here...\",\n",
        "                    show_label=False,\n",
        "                    container=False,\n",
        "                    scale=5\n",
        "                )\n",
        "                send_btn = gr.Button(\"Send\", elem_classes=[\"primary-btn\"], scale=1)\n",
        "\n",
        "    # Connect send button\n",
        "    send_btn.click(\n",
        "        fn=chat_with_bot,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=chatbot\n",
        "    ).then(\n",
        "        fn=lambda: \"\",\n",
        "        inputs=None,\n",
        "        outputs=msg\n",
        "    )\n",
        "\n",
        "    # Connect Enter key\n",
        "    msg.submit(\n",
        "        fn=chat_with_bot,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=chatbot\n",
        "    ).then(\n",
        "        fn=lambda: \"\",\n",
        "        inputs=None,\n",
        "        outputs=msg\n",
        "    )\n",
        "\n",
        "    # Connect experience radio\n",
        "    experience_radio.change(\n",
        "        fn=handle_experience_selection,\n",
        "        inputs=[experience_radio, chatbot],\n",
        "        outputs=chatbot\n",
        "    )\n",
        "\n",
        "\n",
        "     # Reset button definition\n",
        "    clear = gr.Button(\"Reset Conversation\", elem_classes=[\"secondary-btn\"])\n",
        "\n",
        "    # Reset button\n",
        "    clear.click(\n",
        "            fn=reset_conversation,\n",
        "            inputs=None,\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "\n",
        "    # Initialize with greeting when loaded\n",
        "    demo.load(\n",
        "            fn=reset_conversation,\n",
        "            inputs=None,\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "\n",
        "    # Set the specific initial greeting message\n",
        "    initial_greeting = \"\"\"Hello, I am the AI hiring assistant for TalentScout, a recruitment agency specializing in technology placements. I will be guiding you through our application process today.\n",
        "\n",
        "To begin, could you please provide me with your full name?\"\"\"\n",
        "\n",
        "    # Initialize with specific greeting\n",
        "    def set_initial_greeting():\n",
        "        return [{\"role\": \"assistant\", \"content\": initial_greeting}]\n",
        "\n",
        "    demo.load(\n",
        "        fn=set_initial_greeting,\n",
        "        inputs=None,\n",
        "        outputs=chatbot\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3fKl9RJA5lT",
        "outputId": "44fd76c2-a90b-4146-f3ad-e4e9793d21dd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/utils.py:1024: UserWarning: Expected 1 arguments for function <function handle_experience_selection at 0x7d811f428400>, received 2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/utils.py:1032: UserWarning: Expected maximum 1 arguments for function <function handle_experience_selection at 0x7d811f428400>, received 2.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launch the Application"
      ],
      "metadata": {
        "id": "8DYmcJQLBGA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Set initial greeting when loading\n",
        "    chatbot.value = [{\"role\": \"assistant\", \"content\": initial_greeting}]\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "VX0mWAahA3XQ",
        "outputId": "fb46b2f1-4506-47f8-f3d2-aa71fe3cbe3d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d8daac15f153cbcd14.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d8daac15f153cbcd14.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}